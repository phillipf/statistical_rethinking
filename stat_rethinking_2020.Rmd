---
title: "Rethinking Stats"
output: html_document
---
```{r}

# devtools::install_github("stan-dev/cmdstanr")

# cmdstanr::check_cmdstan_toolchain(fix = TRUE)
# cmdstanr::install_cmdstan()

# install.packages(c("coda","mvtnorm","devtools","loo","dagitty"))
# devtools::install_github("rmcelreath/rethinking")
```

##Chapter one

```{r setup, include=FALSE}

grid_size <- 100

calc_posterior <- function(x, size, prior = rep(1, 100), grid.size = grid_size) {

  # define grid
  p_grid <<- seq(from = 0, to = 1, length.out = grid.size)
  # define prior
  # prior <<- prior
  # compute likelihood at each value in grid
  likelihood <<- dbinom(x, size = size, prob = p_grid)
  # compute product of likelihood and prior
  unstd.posterior <<- likelihood * prior
  # standardize the posterior, so it sums to 1
  posterior <<- unstd.posterior / sum(unstd.posterior)

  plot(p_grid, posterior,
    type = "b",
    xlab = "probability of water", ylab = "posterior probability"
  )
  mtext("100 points")
  return(posterior)
}
```

```{r}

calc_posterior( 6 , 9 , grid.size = grid.size )

```

```{r}
prior <- ifelse(p_grid < 0.5, 0, 1)

calc_posterior( 6 , 9 , prior, grid.size=grid.size )

prior <- exp( -5*abs( p_grid - 0.5 ) )

calc_posterior( 6 , 9 , prior, grid.size=grid.size )

calc_posterior(6, 9, prior)
```

```{r}
#Q 2M1
#(1) W, W, W
calc_posterior( 3 , 3 , prior, grid.size=grid.size )
#(2) W, W, W, L
calc_posterior( 3 , 4 , prior, grid.size=grid.size )
#(3) L, W, W, L, W, W, W
calc_posterior( 5 , 7 , prior, grid.size=grid.size )

```

```{r}
# Q 2M2

prior <- ifelse( p_grid < 0.5 , 0 , 1 )

#(1) W, W, W
calc_posterior( 3 , 3 , prior, grid.size=grid.size )
#(2) W, W, W, L
calc_posterior( 3 , 4 , prior, grid.size=grid.size )
#(3) L, W, W, L, W, W, W
calc_posterior( 5 , 7 , prior, grid.size=grid.size )

# (1) W, W, W
calc_posterior(3, 3, prior)
# (2) W, W, W, L
calc_posterior(3, 4, prior)
# (3) L, W, W, L, W, W, W
calc_posterior(5, 7, prior)
```

```{r}

# define prior
prior <- prior
# compute likelihood at each value in grid
likelihood <- dbinom(1, size = 1, prob = 0.3)
# compute product of likelihood and prior
unstd.posterior <- likelihood * prior
# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)
```

##Chapter two




##Chapter three

```{r}
p_grid <- seq(from = 0, to = 1, length.out = 1000)
prior <- rep(1, 1000)
likelihood <- dbinom(6, size = 9, prob = p_grid)
unstd.posterior <- likelihood * prior
posterior <- unstd.posterior / sum(posterior)

sample.size <- 1e4

set.seed(100)
samples <- sample(p_grid, prob = posterior, size = sample.size, replace = TRUE)
```


```{r}
plot(p_grid, posterior)

plot(samples)
```

```{r}
library(rethinking)

dens(samples)
```

### Intervals of defined boundaries

### How much posterior probability lies below some parameter value?

```{r}
# add up posterior probability where p < 0.5
sum(posterior[p_grid < 0.5])

# the same calculation, using samples from the posterior
sum(samples < 0.5) / sample.size
```

### How much posterior probability lies between two parameter values?

```{r}
# how much posterior probability lies between 0.5 and 0.75:

sum(samples > 0.5 & samples < 0.75) / sample.size
```

### Intervals of defined mass

```{r}
# you want to know the boundaries of the lower 80% posterior probability. You know this interval starts at p = 0. To find out where it stops, think of the samples as data and ask where the 80th percentile lies.

quantile(samples, 0.8)

# Similarly, the middle 80% interval lies between the 10th percentile and the 90th percentile. These boundaries are found using the same approach

quantile(samples, c(0.1, 0.9))
```


### 3E1. How much posterior probability lies below p = 0.2?
### 3E2. How much posterior probability lies above p = 0.8?
### 3E3. How much posterior probability lies between p = 0.2 and p = 0.8?

```{r}
`3E1` <- sum(posterior[p_grid < 0.2])
`3E2` <- sum(posterior[p_grid > 0.8])
`3E3` <- sum(posterior[p_grid > 0.2 & p_grid < 0.8])

`3E1` + `3E2` + `3E3`
```

### 3E4. 20% of the posterior probability lies below which value of p?
### 3E5. 20% of the posterior probability lies above which value of p?

```{r}
quantile(samples, 0.2)
quantile(samples, 0.8)
```

### 3E6. Which values of p contain the narrowest interval equal to 66% of the posterior probability?
### 3E7. Which values of p contain 66% of the posterior probability, assuming equal posterior probability both below and above the interval?

```{r}

HPDI(samples, prob = 0.66)

PI(samples, prob = 0.66)
```

### 3M1. Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.

```{r}
n_water <- 8
n <- 15
grid.size <- 1e4

calc_posterior(n_water, n, rep(1, grid.size), grid.size)
```

### 3M2. Draw 10,000 samples from the grid approximation from above. Then use the samples to calculate the 90% HPDI for p.

```{r}

sample.size <- 1e4

set.seed(100)
samples <- sample(p_grid, prob = posterior, size = sample.size, replace = TRUE)

HPDI(samples, 0.9)
```

### 3M3. Construct a posterior predictive check for this model and data. This means simulate the distribution of samples, averaging over the posterior uncertainty in p. What is the probability of observing 8 water in 15 tosses?

```{r}

w <- rbinom( 1e4 , size=15 , prob=samples )

simplehist( w )

p <- table(w)/1e4

p[9]

```

### 3M4. Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses.

```{r}

w <- rbinom( 1e4 , size=9 , prob=samples )

simplehist( w )

p <- table(w)/1e4

p[7]

```

### 3M5. Start over at 3M1, but now use a prior that is zero below p = 0.5 and a constant above p = 0.5.This corresponds to prior information that a majority of the Earth’s surface is water. Repeat each problem above and compare the inferences. What difference does the better prior make? If it helps, compare inferences (using both priors) to the true value p = 0.7.

```{r}

n_water <- 8
n <- 15
grid.size <- 1e4

prior <- ifelse( p_grid < 0.5 , 0 , 1 )

calc_posterior( n_water, n, prior, grid.size )

samples <- sample( p_grid, size=grid.size, prob=posterior, replace = T )

HPDI( samples, 0.9 )

# 3M3

w <- rbinom( 1e4 , size=15 , prob=samples )

simplehist( w )

p <- table(w)/1e4

p[9]

# 3M4

w <- rbinom( 1e4 , size=9 , prob=samples )

simplehist( w )

p <- table(w)/1e4

p[7]

```


##Chapter four 

### coin flipping on the football pitch

```{r}

# runif(16, -1, 1) = uniform distribution between -1 and 1 with 16 observations

pos <- replicate( 1000 , sum( runif(16,-1,1) ) )

plot(density(pos))


```


### 4E1. In the model definition below, which line is the likelihood?
yi ∼ Normal(µ, σ)
µ ∼ Normal(0, 10)
σ ∼ Exponential(1)
### 4E2. In the model definition just above, how many parameters are in the posterior distribution?

```{r}
#4E1
yi <- "likelihood"
µ <- "µ prior"
σ <- "σ prior"

#4E2
# two parameters (µ and σ) in the posterior distribution

```

### 4E. Using the model definition above, write down the appropriate form of Bayes’ theorem that includes the proper likelihood and priors.  

Bayes' theorem = probability of the data * prior / average probability of the data

Normal(µ, σ) * Normal(0, 10) * σ ∼ Exponential(1) /
sum(Normal(µ, σ) * Normal(0, 10) * σ ∼ Exponential(1))

```{r}

#what is bayes' theorem?
# E.g. calculating the pr of 6 waters in 9 tosses
w <- 6; n <- 9;
p_grid <- seq(from=0,to=1,length.out=100)
posterior <- dbinom(w,n,p_grid)*dunif(p_grid,0,1)
posterior <- posterior/sum(posterior)

# E.g. calculating the probabilty of a person being a vampire if they test positive to a vampirism test

Pr_Positive_Vampire <- 0.95
Pr_Positive_Mortal <- 0.01
Pr_Vampire <- 0.001
Pr_Positive <- Pr_Positive_Vampire * Pr_Vampire +
Pr_Positive_Mortal * ( 1 - Pr_Vampire )
( Pr_Vampire_Positive <- Pr_Positive_Vampire*Pr_Vampire / Pr_Positive )



```

##4M1. For the model definition below, simulate observed y values from the prior (not the posterior). I.e. Prior predictive check
yi ∼ Normal(µ, σ)  
µ ∼ Normal(0, 10)  
σ ∼ Exponential(1)  

##4M2. Translate the model just above into a quap formula.

```{r}

#4M1

sample_mu <- rnorm( 1e4 , 0 , 10 )
sample_sigma <- rexp( 1e4 , 1 )
prior_y <- rnorm( 1e4 , sample_mu , sample_sigma )
dens( prior_y )


data(Howell1)
d <- Howell1
d2 <- d[ d$age >= 18 , ]

#4M2

flist <- alist(
  height ~ dnorm( mu , sigma ) ,
  mu ~ dnorm( 0 , 10 ) ,
  sigma ~ dexp( 1 )
)

m4.1 <- quap( flist , data=d2 )

precis( m4.1 )



```

## 4M3. Translate the quap model formula below into a mathematical model definition.

y ~ dnorm( mu , sigma ),
mu <- a + b*x,
a ~ dnorm( 0 , 10 ),
b ~ dunif( 0 , 1 ),
sigma ~ dexp( 1 )

yi ~ Normal( mu, sigma)
mu = a + b*x
a ~ Normal( 0, 10 )
b ~ Uniform( 0, 1 )
sigma ~ Exponential( 1 )

## 4M4. A sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend your choice of priors.

where yi = year of measurement

hi ∼ Normal(µi, σ) [likelihood]
µi = α + β*yi [linear model]
α ∼ Normal(178, 20) [α prior]
β ∼ Normal(0, 10) [β prior]
σ ∼ Uniform(0, 50) [σ prior]

